{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: notes\n",
    "chapter: 4\n",
    "chapter-title: Training Models\n",
    "permalink: /ml-book/chapter4/notes.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "This chapter is all about what's under the hood of models. Understanding this will help us:\n",
    "* Pick the appropriate model\n",
    "* Use the right training algorithm\n",
    "* Adjust hyperparameters\n",
    "* Help with debugging and error analysis\n",
    "* Help with understanding neural networks when we get to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Linear Regression model\n",
    "* Using a direct \"closed-form\" equation that computer the model parameters based on the best fit of the model to the data\n",
    "* Using an iterative optimization approach such as Gradient Descent (GD) which finds parameters by tweaking parameters and minimizing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Regression Models\n",
    "* We'll look at Polynomial Regression which is more complex model, but also prone to overfitting\n",
    "* We'll look at Logistic Regression and Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n $$\n",
    "\n",
    "where:\n",
    "* $\\hat{y}$ is the predicted value\n",
    "* $n$ is the number of features\n",
    "* $x_i$ is the $i^{th}$ feature value\n",
    "* $\\theta_j$ is the $j^{th}$ model parameter ($\\theta_0$ is the bias term and other $\\theta_j$ terms are the feature weights)\n",
    "\n",
    "#### Vectorized Form\n",
    "\n",
    "$$ \\hat{y} = h_{\\theta}(\\textbf{x}) = \\theta^T \\cdot \\textbf{x} $$\n",
    "\n",
    "Note here that $ x_0 = 1 $ to support the bias term $ \\theta_0 $.\n",
    "\n",
    "RMSE for Linear Regression:\n",
    "\n",
    "$$ MSE(\\textbf{X}, h_{\\theta}) = \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^T \\cdot \\textbf{x}^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "The summation is over all the samples in the data set. So i = 1 is the first sample of the dataset, i = 2 the second and so on, up to m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
