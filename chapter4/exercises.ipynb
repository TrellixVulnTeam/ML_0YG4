{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: exercises\n",
    "chapter: 4\n",
    "chapter-title: Training Models\n",
    "permalink: /ml-book/chapter4/exercises.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "You can use any flavor of Gradient Descent (Batch (provided there are few enough samples to fit in memory), Stochastic, or Mini-batch) if you have a training set with millions of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "All forms of Gradient Descent might suffer from features with very different scales. This is because they will converge slower since the gradient descent will take a circuitous route to the minimum as 4-7 illustrates. I like to think about this as an ellipse and the gradient descent goes towards the semi major axis first, once it reaches there it heads along it towards the minimum. To fix this you can standard scale or min max scale the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "No, the log loss function for Logistic Regression is convex so there's no need to worry about it getting stuck in a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Not all Gradient Descent algorithms lead to the exact same model provided you let them run long enough. Most of the time it's close enough, but Batch Gradient descent will converge smoothly to a specific minimum. However, Stochastic Gradient Descent and Mini-Batch Gradient Descent may \"bounce\" around the global minimum. However, if you gradually lower the learning rate they will become closer and closer to BGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "If the validation error goes up at every epoch you're likely overfitting. There are various methods to prevent overfitting including: Use a less complicated model, use Ridge, Lasso, or Elastic Net regularization, increase the size of your (training) dataset. Geron notes that it could also be that the learning rate is too high this would definitely be the case if the training error is going up as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Typically to implement early stopping you would have some patience factor. I don't think it would be a good idea to stop immediately because of this. For example, the validation error could be consistently going down, increase for one epoch, and then go down for the next 10 epochs. The patience factor says something like \"if _ epochs go by without a decrease in the validation error, then stop (and revert to minimum validation error model)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "The normal equations will be fast when the number of features is low because it is linear in the number of samples, it will converge exactly. Batch Gradient Descent will be slow for a large number of samples, but largely unaffected by the number of features, it will converge exactly. Stochastic Gradient Descent and Mini-batch Gradient Descent will both be fast, but require that the learning rate be decreases so that they actually converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Three ways to solve a gap between the training error and validation error in Polynomial Regression are: Increase size of the dataset, apply regularization, use a less complicated model. Overfitting is what is happening. Nailed this answer : )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "If the training error and validation error are almost equal and fairly high then that indicates high bias. You should reduce the regularization parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "\n",
    "* Ridge Regression instead of Linear Regression?\n",
    "  * You want to prevent overfitting (high variance) in your model.\n",
    "* Lasso instead of Ridge Regression?\n",
    "  * You want to completely eliminate the impact of the least important features instead of just penalizing them.\n",
    "* Elastic Net instead of Lasso?\n",
    "  * You want to reduce the complexity of your model and only use the most important features, but want to avoid the erratic behavior of Lasso (when # features > # samples or several features are strongly correlated).\n",
    "  * Good tip from Geron: If you want to use Lasso without the negative effects just use Elastic Net with an l1 ratio close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "\n",
    "If you want to classify pictures as outdoor/indoor and daytime/nighttime, then you should implement two Logistic Regression classifiers instead of one Softmax Regression classifier. Softmax Regression is multi-class, not multi-output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "\n",
    "Implement Batch Gradient Descent with early stopping for Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sounds difficult, but let's give it a shot\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105 135 150]\n",
      "(105, 4) (105, 1)\n",
      "(30, 4) (30, 1)\n",
      "(15, 4) (15, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = iris[\"data\"], iris[\"target\"]\n",
    "arr = np.hstack((X, y[np.newaxis, :].T))\n",
    "np.random.seed(42)\n",
    "p = np.random.permutation(len(arr))\n",
    "arr = arr[p]\n",
    "s = (np.array([0.7, 0.2, 0.1]) * len(arr)).astype(int)\n",
    "s = np.cumsum(s)\n",
    "print(s)\n",
    "X_train, y_train = arr[0:s[0], :-1], arr[0:s[0], -1][:, np.newaxis]\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_val, y_val = arr[s[0]:s[1], :-1], arr[s[0]:s[1], -1][:, np.newaxis]\n",
    "print(X_val.shape, y_val.shape)\n",
    "X_test, y_test = arr[s[1]:s[2], :-1], arr[s[1]:s[2], -1][:, np.newaxis]\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.68\n",
      "Loss:  0.56\n",
      "Loss:  0.5\n",
      "Loss:  0.46\n",
      "Loss:  0.43\n",
      "Loss:  0.41\n",
      "Loss:  0.39\n",
      "Loss:  0.38\n",
      "Loss:  0.37\n",
      "Loss:  0.35\n"
     ]
    }
   ],
   "source": [
    "def softmax_eval(X, y, theta):\n",
    "    y = y[:, 0]\n",
    "    m = X.shape[0]\n",
    "    K = len(set(y))\n",
    "    l = 0\n",
    "    for k in range(K):\n",
    "        y_tmp = (y == k).astype(int)\n",
    "        for i in range(m):\n",
    "            s_k = theta[:, k].T @ X[i]\n",
    "            p_k = np.exp(s_k) / sum(np.exp(theta[:, k].T @ X[i]) for k in range(K))\n",
    "            l += y_tmp[i] * np.log(p_k)\n",
    "    l = (-1 / m) * l\n",
    "    return l\n",
    "        \n",
    "\n",
    "def train(X, y, iters=1000, lr=0.01):\n",
    "    y = y[:, 0]\n",
    "    m = X.shape[0]\n",
    "    K = len(set(y))\n",
    "    theta = np.ones((X.shape[1], K))\n",
    "    eta = lr\n",
    "    for iteration in range(iters):\n",
    "        for k in range(K):\n",
    "            y_tmp = (y == k).astype(int)\n",
    "            grad_k = 0\n",
    "            for i in range(m):\n",
    "                s_k = theta[:, k].T.dot(X[i])\n",
    "                p_k = np.exp(s_k) / sum(np.exp(theta[:, k].T @ X[i]) for k in range(K))\n",
    "                grad_k += (p_k - y_tmp[i]) * X[i]\n",
    "            grad_k = (1 / m)*grad_k\n",
    "            theta[:, k] = theta[:, k] - eta * grad_k\n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            l = softmax_eval(X, y.reshape((-1, 1)), theta)\n",
    "            print(\"Loss: \", round(l, 2))\n",
    "    return theta\n",
    "\n",
    "theta = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was my first attempt... looking at Geron's work there's still a little to do here.\n",
    "The main difference is that he vectorized everything. He also added the bias term which I forgot to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    K = len(set(y[:, 0]))\n",
    "    new_y = np.zeros((len(y), K))\n",
    "    i, j = np.indices(new_y.shape)\n",
    "    return (j == y).astype(int)\n",
    "    \n",
    "y_train_hot = one_hot(y_train)\n",
    "y_val_hot = one_hot(y_val)\n",
    "y_test_hot = one_hot(y_test)\n",
    "\n",
    "# Add bias\n",
    "X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "X_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, iters=1000, lr=0.01):\n",
    "    y = y[:, 0]\n",
    "    m = X.shape[0]\n",
    "    K = len(set(y))\n",
    "    theta = np.ones((X.shape[1], K))\n",
    "    eta = lr\n",
    "    for iteration in range(iters):\n",
    "        # TODO\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for k in range(K):\n",
    "            y_tmp = (y == k).astype(int)\n",
    "            grad_k = 0\n",
    "            for i in range(m):\n",
    "                s_k = theta[:, k].T.dot(X[i])\n",
    "                p_k = np.exp(s_k) / sum(np.exp(theta[:, k].T @ X[i]) for k in range(K))\n",
    "                grad_k += (p_k - y_tmp[i]) * X[i]\n",
    "            grad_k = (1 / m)*grad_k\n",
    "            theta[:, k] = theta[:, k] - eta * grad_k\n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            l = softmax_eval(X, y.reshape((-1, 1)), theta)\n",
    "            print(\"Loss: \", round(l, 2))            \n",
    "        # Early stopping portion\n",
    "        new_val_loss = softmax_eval(X_v, y_v, theta)\n",
    "        if new_val_loss <= best_val_loss:\n",
    "            best_val_loss = new_val_loss\n",
    "            best_theta = theta\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 10:\n",
    "                return best_theta, best_val_loss\n",
    "    print(\"Warning: did not Early Stop...\")\n",
    "    return best_theta, best_val_loss\n",
    "\n",
    "# theta, val_loss = train_with_early_stopping(X_train, y_train, X_val, y_val, lr=11, iters=100000)\n",
    "# theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
