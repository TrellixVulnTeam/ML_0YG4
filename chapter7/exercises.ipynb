{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: exercises\n",
    "chapter: 7\n",
    "chapter-title: Ensemble Learning and Random Forests\n",
    "permalink: /ml-book/chapter7/exercises.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "If I've trained five different models on the exact same training data and they all achieve 95% precision there is a chance that I could combine these models to get better results. You would combine the 5 predictions and predict the majority vote and that could achieve a precision higher than 95%.\n",
    "\n",
    "Accuracy: (TP + TN) / (FP + FN + TP + TN)\n",
    "Precision: TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "A hard vote counts the weight of each vote equally when determining the majority vote, whereas a soft voting classifier will weight each vote according to it's `predict_proba` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Bagging is when you sample with replacement, so it would definitely be possible to distribute across multiple servers, you just need to copy all the data to each server. I think it would also be possible to distribute pasting ensembles across several servers, again just put a copy of the data on each server and sample without replacement. Boosting sounds difficult to spread across servers because each model needs to correct it's predecessor. Yes random forests are possible to distribute. Stacking would be partly possible since the parts before the blender could be distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Out-of-bag evaluation allows you to gain an idea of how a bagging algorithm will perform on new data, because roughly 37% of data will not have been used in training the model. Therefore, this portion of the training sample can be used for evaluation much like a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Extra-Trees are more random than Random Forests because instead of choosing the optimal feature/threshold at each node (i.e. the feature (amongst a random subset) and threshold which best splits the data at the node), it randomly selects a threshold for each feature. Therefore, the resultant tree is less predictable since each split was made randomly based on these thresholds. They're faster than Random Forests since they don't require time to compute the optimal feature/threshold for each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "If your AdaBoost ensemble underfits the training data you should increase the learning rate (defaults to 1). You can also try increasing the number of estimators or decreasing the default regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "If your Gradient Boosting ensemble overfits then you should decrease the learning rate so that it will generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Load the MNIST data and split it into a training set, a validation set, and a test set (50k (typo in book), 10k, 10k). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From chapter 3:\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# MNIST changed to https://www.openml.org/d/554\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (10000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_train, X_inter, y_train, y_inter = train_test_split(X, y, test_size=20_000)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_inter, y_inter, test_size=10_000)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "rfc, etc, lsvc = (\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    ExtraTreesClassifier(random_state=42),\n",
    "    LinearSVC(random_state=42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc done\n",
      "etc done\n",
      "lsvc done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riley/PycharmProjects/ML/venv/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "print(\"rfc done\")\n",
    "etc.fit(X_train, y_train)\n",
    "print(\"etc done\")\n",
    "lsvc.fit(X_train, y_train)\n",
    "print(\"lsvc done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=42, tol=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(tol=1, random_state=42)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=100, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"rfc\", rfc),\n",
    "    (\"etf\", etc),\n",
    "    (\"svc\", svc),\n",
    "    (\"ada\", ada),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc 0.9667\n",
      "etf 0.9696\n",
      "svc 0.9784\n",
      "ada 0.7274\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model[0], model[1].score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9697"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vc = VotingClassifier(models, voting=\"hard\", n_jobs=6)\n",
    "vc.fit(X_train, y_train)\n",
    "vc.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9737"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remove ada\n",
    "models = [\n",
    "    (\"rfc\", rfc),\n",
    "    (\"etf\", etc),\n",
    "    (\"svc\", svc),\n",
    "]\n",
    "vc = VotingClassifier(models, voting=\"hard\", n_jobs=6)\n",
    "vc.fit(X_train, y_train)\n",
    "vc.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9792"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Still not better than the SVM let's try the soft\n",
    "svc.probability = True\n",
    "models[2][1].probability = True\n",
    "vc_soft = VotingClassifier(models, voting=\"soft\", n_jobs=6)\n",
    "vc_soft.fit(X_train, y_train)\n",
    "vc_soft.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VC Soft Score: 0.9767\n",
      "RandomForestClassifier: 0.9647\n",
      "ExtraTreesClassifier: 0.9676\n",
      "SVC: 0.976\n"
     ]
    }
   ],
   "source": [
    "# So we did better, let's try on test set\n",
    "print(\"VC Soft Score:\", vc_soft.score(X_test, y_test))\n",
    "for model in vc_soft.estimators_:\n",
    "    print(model.__class__.__name__ + \":\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we did a very little bit better. Our soft voting classifier had 97.67% accuracy and our SVC alone had 97.60% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image's class. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let's evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.10000000e-01 0.00000000e+00 0.00000000e+00 ... 7.79371387e-07\n",
      "  1.09459623e-06 3.72442020e-06]\n",
      " [9.50000000e-01 0.00000000e+00 1.00000000e-02 ... 4.23279470e-05\n",
      "  5.95567083e-05 1.67694188e-03]\n",
      " [8.70000000e-01 0.00000000e+00 0.00000000e+00 ... 1.10429014e-03\n",
      "  4.04542473e-05 2.90887552e-04]\n",
      " ...\n",
      " [0.00000000e+00 1.00000000e-02 3.00000000e-02 ... 4.31194473e-05\n",
      "  1.44719826e-03 1.08973280e-04]\n",
      " [2.00000000e-02 0.00000000e+00 6.00000000e-02 ... 2.61564119e-04\n",
      "  1.06890698e-03 2.14030781e-04]\n",
      " [3.00000000e-02 1.00000000e-02 2.00000000e-02 ... 1.05278615e-02\n",
      "  6.03766703e-03 5.01682195e-01]]\n",
      "(10000, 30) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_new = -np.ones((X_val.shape[0], 10*len(vc_soft.estimators_)))\n",
    "X_new[:, :10] = vc_soft.estimators_[0].predict_proba(X_val)\n",
    "X_new[:, 10:20] = vc_soft.estimators_[1].predict_proba(X_val)\n",
    "X_new[:, 20:] = vc_soft.estimators_[2].predict_proba(X_val)\n",
    "y_new = y_val\n",
    "print(X_new)\n",
    "print(X_new.shape, y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(X_new, y_new)\n",
    "mlp.score(X_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 1.17529491e-09\n",
      "  9.99998859e-01 1.11848133e-06]\n",
      " [4.00000000e-02 1.70000000e-01 4.00000000e-02 ... 9.16153445e-03\n",
      "  5.74427818e-01 9.49229029e-03]\n",
      " [0.00000000e+00 0.00000000e+00 3.00000000e-02 ... 9.79965813e-01\n",
      "  4.73102141e-04 6.88522020e-03]\n",
      " ...\n",
      " [1.00000000e-02 4.00000000e-02 1.10000000e-01 ... 1.12631510e-05\n",
      "  9.98781514e-01 4.08627716e-04]\n",
      " [0.00000000e+00 3.40000000e-01 0.00000000e+00 ... 4.80205378e-01\n",
      "  8.56538302e-03 7.77163413e-02]\n",
      " [1.80000000e-01 0.00000000e+00 3.00000000e-02 ... 6.09119548e-03\n",
      "  6.08639737e-02 8.56811451e-03]]\n",
      "(10000, 30) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_new_test = -np.ones((X_test.shape[0], 10*len(vc_soft.estimators_)))\n",
    "X_new_test[:, :10] = vc_soft.estimators_[0].predict_proba(X_test)\n",
    "X_new_test[:, 10:20] = vc_soft.estimators_[1].predict_proba(X_test)\n",
    "X_new_test[:, 20:] = vc_soft.estimators_[2].predict_proba(X_test)\n",
    "y_new_test = y_test\n",
    "print(X_new_test)\n",
    "print(X_new_test.shape, y_new_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_new_test, y_new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we did a very little bit worse with the stacking ensemble. Our soft voting classifier had 97.67% accuracy and our stacking ensemble had 97.46% accuracy on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
