{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: notes\n",
    "chapter: 7\n",
    "chapter-title: Ensemble Learning and Random Forests\n",
    "permalink: /ml-book/chapter7/notes.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A group of predictors is called an ensemble\n",
    "* An ensemble of decision trees is known as a Random Forest\n",
    "* This so called wisdom of the crowd approach of combining multiple models into an ensemble (or even multi ensembles of models) account for some of the best solutions in Machine Learning\n",
    "* Popular ensemble methods include bagging, boosting, stacking, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "* A voting classifier uses the predictions of multiple diverse predictors (models) and has each one vote and then picks the majority vote\n",
    "* This is called a hard voting classifier\n",
    "* You can achieve high accuracy from a bunch of weak learners\n",
    "* \"Suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% o the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy!\"\n",
    "* However in practice this is only true if the classifiers are all independent of one another, which isn't the case since they were all trained on the same data\n",
    "* Ensemble methods work best when the predictors are as independent from one another as possible (e.g. use different training algorithms). This will allow them to make different types of errors, increasing the models accuracy.\n",
    "* Soft voting is when you use predictors which have the `predict_proba` method (i.e. they predict probabilities). You pick the class with the highest class probability, averaged over all the individual classifiers. This is often more effective because it gives higher weight to higher confidence votes, whereas in hard voting they are all equally weighted (even though some individual models may be unconfident in the result).\n",
    "\n",
    "## Bagging and Pasting\n",
    "* Instead of using completely different algorithms to produce an ensemble model, another way to do so is to train the same algorithm on different random subsets of the training set\n",
    "* This technique is known as bagging and pasting\n",
    "* Bagging (short for bootstrap aggregating) is when you sample randomly from the training set with replacement\n",
    "* Pasting is when you sample randomly from the training set without replacement\n",
    "\n",
    "![image](https://miro.medium.com/max/503/1*z4GrjL9vVGv0PU9Fk1W8zw.png)\n",
    "\n",
    "* The statistical mode is used for aggregating votes\n",
    "* This aggregation reduces both bias and variance\n",
    "* Generally the ensemble has a similar bias, but a lower variance than a single predictor trained on the original training set\n",
    "* These methods scale very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'echo'\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "# What this looks like:\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, \n",
    "                            bootstrap=True, # False would be w/o replacement\n",
    "                           n_jobs=4)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on\n",
    "  * Bagging ends up with a slightly higher bias than pasting, but thisalso means that predictors end up being less correlated so the ensemble's variance is reduced\n",
    "* Overall, bagging often results in better models, which explains why it is generally preferred\n",
    "* You can evalute both bagging and pasting if you have time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "* https://stats.stackexchange.com/questions/88980/why-on-average-does-each-bootstrap-sample-contain-roughly-two-thirds-of-observat\n",
    "* https://stats.stackexchange.com/questions/173520/random-forests-out-of-bag-sample-size\n",
    "* If you sample randomly with replacement it turns out that for each predictors about 37% of the samples will not be selected (for large enough number of samples m)\n",
    "* These unsampled samples are known as out-of-bag instances\n",
    "* Thus by performing validation on the oob instances and averaging across predictors we can get an estimate of the accuracy on the test set\n",
    "* So it serves as like a free validation set `oob_score=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the decision boundary created by the tree:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/29719483/173130302-13031246-b489-4d2d-a891-088bff582ae3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Trees are considered a white box model (as opposed to black box) because the it's easy to retrace why decisions were made by the model, whereas in neural network that is not as feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities\n",
    "* It assigns probabilities simply based on the fraction of instances for each class that showed up in the associated node in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.90740741 0.09259259]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(tree_clf.predict_proba([[5, 1.5]]))\n",
    "print(tree_clf.predict([[5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm\n",
    "* The CART algorithm works as follows:\n",
    "  * At the root node pick a feature and a threshold for that feature which splits the dataset in half\n",
    "    * Pick the feature threshold combo which results in the purest subsets (weighted by their size)\n",
    "  * Continue recursively down the tree until some user defined stop condition is met (i.e. max_depth) or it cannot find an additional split which will reduce the impurity.\n",
    "* This is a Greedy algorithm in the sense that it starts with an optimal split, and continues to search for it at each level, but it doesn't consider the impurity of lower levels when making a decision for the current level\n",
    "* In general the optimal tree is NP-Complete $O(exp(m))$ and intractable, in general the CART algorithm produces a reasonably good solution\n",
    "\n",
    "## Computational Complexity\n",
    "* It's essential a binary search tree which is $O(log(m))$ to traverse for predictions\n",
    "* For training, since it compares all features on all samples at each node it is $O(n \\times log(m))$\n",
    "\n",
    "## Gini Impurity or Entropy?\n",
    "* Entropy is another impurity measure\n",
    "* A set's entropy is zero when it contains instances of only one class (which is also when gini is 0)\n",
    "\n",
    "$$H_i = - \\sum\\limits_{\\substack{k=1 \\\\ p_{i,k} \\neq 0}}^n p_{i,k} log(p_{i,k})$$\n",
    "\n",
    "* **Most of the time it doesn't matter which metric you use. Gini impurity is slightly faster to compute, so it's a good default. When they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.**\n",
    "\n",
    "## Regularization Hyperparameters\n",
    "* Decision trees left unconstrained are liable to overfit the training data\n",
    "* They are known as nonparametric models because their parameters are not pre-defined such as in a linear model\n",
    "* To prevent overfitting, we can use regularization to restrict the degrees of freedom of the decision tree\n",
    "* There are several regularization parameters to pick from\n",
    "* Increasing the `min` hyperparameters and reducing the `max` will regularize the model\n",
    "\n",
    "* Other algorithms work by creating an unrestricted tree and then pruning unnecessary nodes\n",
    "* Nodes are deemed unnecessary if the children of a node are both end leaves and the p-value dictates that the chances of that split are statiscally insignifcant\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/29719483/173135440-072bd762-ae06-40d7-9c9c-d56455ecf987.png)\n",
    "\n",
    "## Regression\n",
    "* Find an x value that splits the training targets in half and continue splitting until stopping condition, the target value is the mean of the target value of the samples in that node.\n",
    "* I think this is kind of dumb... like when are results from this sort of regression going to be better... boundaries are only orthogonal\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/29719483/173137358-b3b5849c-2ea4-4f87-976e-2c6ca0af64dd.png)\n",
    "\n",
    "## Instability\n",
    "* Only orthogonal decision boundary which makes them susceptible to rotations in the training data\n",
    "* Can use PCA to mitigate this issue\n",
    "* Very sensitive to small variations in the training data\n",
    "* Removing one sample from the Iris dataset results in a totally different tree, even without removing samples the resultant tree might be different just because of the stochastic nature of the Scikit-learn algorithm\n",
    "* Random Forests can limit this instability by averaging predictions over many trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
