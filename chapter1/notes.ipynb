{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: notes\n",
    "chapter: 1\n",
    "chapter-title: The Machine Learning Landscape\n",
    "permalink: /ml-book/chapter1/notes.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Most of this we already know..._\n",
    "\n",
    "## What is machine learning?\n",
    "\n",
    "* Machine learning is the process of enabling a machine to solve a particular problem without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use machine learning?\n",
    "\n",
    "* Existing solutions to a problem are vast and tedious to program\n",
    "* There is no known solution or the solution is sufficiently complicated to be solved programmatically/ analytically\n",
    "* You expect data to change. Machine learning models are capable of adapting to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the different types of machine learning?\n",
    "\n",
    "* <ins>Supervised Learning</ins> - features of data go in along with labelled output, training occurs, and the model predicts the output give new input data\n",
    "* Unsupervised learning - samples are not labelled, think clustering algorithms etc.\n",
    "* Semisupervised learning - A few of the samples are labelled (e.g. unlabelled samples could be classified as belonging to the same cluster as the labelled sample)\n",
    "* Reinforcement learning - An agent observes an environment and learns to perform actions which optimize for rewards over penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for learning\n",
    "\n",
    "* Batch learning - An entire dataset is trained on, the model is launched into production, and it simply makes predictions on the new incoming data. The model doesn't change/learn anymore, unless it is pulled offline and trained again on a new set of data. Becomes difficult if you have a lot of data or data is varying often.\n",
    "* Online learning - Learning occurs sequentially by consuming mini-batches of data. Drawbacks of this include bad data (where the model adjusts to quickly and performs poorly) or a learning rate which is too slow (model doesn't adjust quick enough to new data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways the machine learning models generalize\n",
    "\n",
    "* Instance-based learning - Memorize learning instances (i.e. samples), determine how similar a new instance is to existing data, act accordingly\n",
    "* Model-based learning - You decide an equation, learn the free parameters of that equation, and then use the equation to make predictions on new data. Think linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of machine learning\n",
    "\n",
    "* Insufficient data - If you don't have enough data your model won't generalize well\n",
    "* Nonrepresentative training data - Obviously if you don't give it data similar to what you want it to predict, it won't stand a chance.\n",
    "* Sampling bias - The data you collect is biased. That would be like me feeding in only data from only west coast teams to a sports algorithm because those are teams I care about the most (_for instance_). \n",
    "* Lousy data - Errors/inconsistencies in training data.\n",
    "* Irrelevant features - You feed in data that does not impact the result. Feature engineering is the practice of discerning which features/data to use.\n",
    "* Overfitting - Model doesn't generalize to new data because it \"memorizes\" a perfect/near-perfect model for training data\n",
    "* Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "* Leave behind some of the training data to test as input into your model. This will give you insight into how your model generalizes. A common split is 80/20, train/test. If you are trying to determine the best hyperparameters it is useful to also make a 3rd partition of the data known as the validation set. That way you are not selecting hyperparameters based on the test set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
